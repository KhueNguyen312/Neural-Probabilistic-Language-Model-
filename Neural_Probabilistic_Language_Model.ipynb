{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Neural Probabilistic Language Model ",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyaoranClone/Neural-Probabilistic-Language-Model-/blob/master/Neural_Probabilistic_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-l-4iPSChod",
        "colab_type": "code",
        "outputId": "9f86ec4b-f69a-4fe6-9f1d-7846addbcd60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import gensim as gs\n",
        "import nltk\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "import random\n",
        "import collections\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A16WdyNYGztg",
        "colab_type": "code",
        "outputId": "159d792d-a180-48d9-e1d8-3e15409d7750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhbp_7FbKb-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import gutenberg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifR-JtghK11G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_words  = gutenberg.words('shakespeare-macbeth.txt')\n",
        "data_sents  = gutenberg.sents('shakespeare-macbeth.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YaWvr_sW02m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dictionaries(words):\n",
        "  count = collections.Counter(words).most_common()\n",
        "  dictionary = dict()\n",
        "  for word, _ in count:\n",
        "    dictionary[word] = len(dictionary) #len(dictionary) increases each iteration\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "  return dictionary, reverse_dictionary\n",
        " \n",
        "dictionary, reverse_dictionary = build_dictionaries(data_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3_5BUDzlxlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(dictionary)\n",
        "embedding_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXc0Rmci3VZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(data,max_len,step=1):\n",
        "  sections = []\n",
        "  section_labels = []\n",
        "  for i in range(0,len(data)-max_len,step):\n",
        "    sections.append(data[i:i+max_len])\n",
        "    section_labels.append([data[i+max_len]])\n",
        "    \n",
        "  train_x = np.zeros((len(sections),max_len,1))\n",
        "  train_y = np.zeros((len(sections),vocab_size))\n",
        "  \n",
        "  for i,section in enumerate(sections):\n",
        "    for j,word in enumerate(section):\n",
        "      train_x[i,j] = dictionary[word]\n",
        "    train_y[i,dictionary[section_labels[i][0]]] = 1.0\n",
        "  p_train_x = []\n",
        "  for i,_ in enumerate(train_x):\n",
        "    p_train_x.append(np.reshape(np.array(train_x[i]), [-1, max_len])[0])\n",
        "  return p_train_x,train_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8HO5-F4WSSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM:\n",
        "  def __init__(self,input_size,embbeding_size,hidden_nodes,learning_rate):\n",
        "    self.lr = learning_rate\n",
        "    self.eb_size = embbeding_size\n",
        "    self.x = tf.placeholder(tf.int32, [None ,input_size])\n",
        "    #self.ex = tf.placeholder(tf.float32,[None,vocab_size])\n",
        "    self.y = tf.placeholder(tf.float32,[None,vocab_size])\n",
        "    \n",
        "    #w1 b1 embedding layer\n",
        "#     self.w1 = tf.Variable(tf.random_normal([vocab_size, self.eb_size]))\n",
        "#     self.b1 = tf.Variable(tf.random_normal([self.eb_size]))\n",
        "#     self.hidden_representation = tf.add(tf.matmul(self.ex,self.w1), self.b1)\n",
        "#     inputs = tf.nn.embedding_lookup(self.hidden_representation,self.x)\n",
        "\n",
        "    embedding = tf.Variable(tf.random_uniform((vocab_size, self.eb_size), -1, 1))\n",
        "    embed = tf.nn.embedding_lookup(embedding, self.x) # use tf.nn.embedding_lookup to get the hidden layer\n",
        "    \n",
        "    self.w = tf.Variable(tf.random_normal([hidden_nodes,vocab_size]))\n",
        "    self.b = tf.Variable(tf.random_normal([vocab_size]))\n",
        "    \n",
        "    #build RNN layer\n",
        "    with tf.name_scope(\"RNN_Layers\"):\n",
        "      cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(hidden_nodes,reuse=tf.AUTO_REUSE),tf.contrib.rnn.BasicLSTMCell(hidden_nodes,reuse=tf.AUTO_REUSE)]) \n",
        "      #tf.contrib.rnn.BasicLSTMCell(hidden_nodes,forget_bias=1.0,reuse=tf.AUTO_REUSE)\n",
        "      output,state = tf.contrib.rnn.static_rnn(cell,tf.unstack(embed,input_size,1),dtype=tf.float32)\n",
        "    \n",
        "    self.logits = tf.matmul(output[-1],self.w)+self.b\n",
        "    self.prediction = tf.nn.softmax(self.logits)\n",
        "    self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits,labels=self.y))\n",
        "    self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
        "    self.train_op = self.optimizer.minimize(self.loss)\n",
        "    \n",
        "    with tf.name_scope(\"Accuracy\"):\n",
        "      correct_pred = tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.y, 1))\n",
        "      accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "  \n",
        "  #This function is used to get a random word from the prediction according to its probabilities.\n",
        "  def sample(self,preds,temperature=1.0):\n",
        "    if temperature <= 0:\n",
        "      return np.argmax(preds)\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    print(probas)\n",
        "    return np.argmax(probas)\n",
        "    \n",
        "  def generate_text_from_trained_data(self,data,prediction_length,sess):\n",
        "    start_index = random.randint(0, len(data) - max_len - 1)\n",
        "    test_start = data[start_index: start_index + max_len]\n",
        "    result = test_start\n",
        "    X_test = [[dictionary[word] for word in test_start]]\n",
        "#     X_test = np.zeros((1,max_len,self.eb_size))\n",
        "#     for i,word in enumerate(test_start):\n",
        "#       X_test[0,i] = word_model[word]\n",
        "      \n",
        "    print(\" Result:\")\n",
        "    for _ in range(prediction_length): \n",
        "      pred = sess.run(self.prediction,feed_dict={self.x:X_test})\n",
        "      #pred = pred.reshape(-1)\n",
        "      pred = int(tf.argmax(pred,1).eval())\n",
        "      #print(int(tf.argmax(pred,1).eval()))\n",
        "      #pred = self.sample(pred,temperature=0.7)\n",
        "      #print(pred)\n",
        "      #next_word = word_model.wv.most_similar(positive=[pred])[0][0]\n",
        "      next_word = reverse_dictionary[pred]\n",
        "      #print(next_word)\n",
        "      test_start.append(next_word)\n",
        "      result.append(next_word)\n",
        "      \n",
        "      test_start = test_start[-max_len:]\n",
        "      X_test = [[dictionary[word] for word in test_start]]\n",
        "#       X_test = np.zeros((1,max_len,self.eb_size))\n",
        "#       for j,word in enumerate(test_start):\n",
        "#         X_test[0,j] = word_model[word]\n",
        "    print(*result)\n",
        "\n",
        "        \n",
        "\n",
        "  def next_batch(self, data, batch_start, batch_size):\n",
        "        len_data = len(data)\n",
        "        batch_end = min(batch_start + batch_size, len_data)\n",
        "        return data[batch_start:batch_end]\n",
        "      \n",
        "  def train(self,splited_data,x_data,y_data,num_eponches=1,batch_size=1,prediction_length=50,checkpoint_path=\"/content/gdrive/My Drive/Colab Notebooks/Trained Models/\"):\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as session:\n",
        "      len_data = len(x_data)\n",
        "      if tf.train.latest_checkpoint(checkpoint_path):\n",
        "        print(\"Restore checkpoint.\")\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(session, tf.train.latest_checkpoint(checkpoint_path))\n",
        "      else:\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        print(\"No checkpoint found! start a new training...\")\n",
        "      for epoch in range(1, num_eponches + 1):\n",
        "                epoch_start_time = time.time()\n",
        "                for batch_start in range(0, len_data, batch_size):\n",
        "                    optimiser_value, loss_value = session.run([self.train_op,self.loss], feed_dict={\n",
        "                        self.x: self.next_batch(x_data, batch_start, batch_size),\n",
        "                        self.y: self.next_batch(y_data, batch_start, batch_size)})\n",
        "                    sys.stdout.write(\n",
        "                        \"\\rEpoch: {}/{}, Batch: {}/{}, Training loss: {}\".format(\n",
        "                            epoch,\n",
        "                            num_eponches,\n",
        "                            (batch_start / batch_size) + 1,\n",
        "                            len_data / batch_size,\n",
        "                            np.mean(loss_value)\n",
        "                        )\n",
        "                    )\n",
        "                    sys.stdout.flush()\n",
        "                    \n",
        "                    #self.generate_text_from_trained_data(splited_data,prediction_length,session)\n",
        "                print (\" Time: {} s\".format(time.time() - epoch_start_time))\n",
        "                model_name = \"cp_language_model.ckpt\"\n",
        "                saver.save(session, checkpoint_path + model_name)\n",
        "                self.generate_text_from_trained_data(splited_data,prediction_length,session)\n",
        "                \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBdF7pbcfkTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class biLSTM:\n",
        "  def __init__(self,input_size,embbeding_size,hidden_nodes,learning_rate):\n",
        "    self.lr = learning_rate\n",
        "    self.eb_size = embbeding_size\n",
        "    self.x = tf.placeholder(tf.int32, [None ,input_size])\n",
        "    self.y = tf.placeholder(tf.float32,[None,vocab_size])\n",
        "    \n",
        "    embedding = tf.Variable(tf.random_uniform((vocab_size, self.eb_size), -1, 1))\n",
        "    embed = tf.nn.embedding_lookup(embedding, self.x) # use tf.nn.embedding_lookup to get the hidden layer\n",
        "    \n",
        "    self.w = tf.Variable(tf.random_normal([2*hidden_nodes,vocab_size])) # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
        "    self.b = tf.Variable(tf.random_normal([vocab_size]))\n",
        "    \n",
        "    #build RNN layer\n",
        "    with tf.name_scope(\"RNN_Layers\"):\n",
        "      # Forward direction cell\n",
        "      lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(hidden_nodes, forget_bias=1.0,reuse=tf.AUTO_REUSE)\n",
        "      # Backward direction cell\n",
        "      lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(hidden_nodes, forget_bias=1.0,reuse=tf.AUTO_REUSE)\n",
        "      output, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell,tf.unstack(embed,input_size,1),dtype=tf.float32)\n",
        "      \n",
        "    \n",
        "    self.logits = tf.matmul(output[-1],self.w)+self.b\n",
        "    self.prediction = tf.nn.softmax(self.logits)\n",
        "    self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits,labels=self.y))\n",
        "    self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
        "    self.train_op = self.optimizer.minimize(self.loss)\n",
        "    \n",
        "    with tf.name_scope(\"Accuracy\"):\n",
        "      correct_pred = tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.y, 1))\n",
        "      accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "  \n",
        "  #This function is used to get a random word from the prediction according to its probabilities.\n",
        "  def sample(self,preds,temperature=1.0):\n",
        "    if temperature <= 0:\n",
        "      return np.argmax(preds)\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    print(probas)\n",
        "    return np.argmax(probas)\n",
        "    \n",
        "  def generate_text_from_trained_data(self,data,prediction_length,sess):\n",
        "    start_index = random.randint(0, len(data) - max_len - 1)\n",
        "    test_start = data[start_index: start_index + max_len]\n",
        "    result = test_start\n",
        "    X_test = [[dictionary[word] for word in test_start]]\n",
        "      \n",
        "    print(\" Result:\")\n",
        "    for _ in range(prediction_length): \n",
        "      pred = sess.run(self.prediction,feed_dict={self.x:X_test})\n",
        "      pred = int(tf.argmax(pred,1).eval())\n",
        "      next_word = reverse_dictionary[pred]\n",
        "      test_start.append(next_word)\n",
        "      result.append(next_word)\n",
        "      \n",
        "      test_start = test_start[-max_len:]\n",
        "      X_test = [[dictionary[word] for word in test_start]]\n",
        "    print(*result)\n",
        "\n",
        "        \n",
        "\n",
        "  def next_batch(self, data, batch_start, batch_size):\n",
        "        len_data = len(data)\n",
        "        batch_end = min(batch_start + batch_size, len_data)\n",
        "        return data[batch_start:batch_end]\n",
        "      \n",
        "  def train(self,splited_data,x_data,y_data,num_eponches=1,batch_size=1,prediction_length=50,checkpoint_path=\"/content/gdrive/My Drive/Colab Notebooks/Trained Models/\"):\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as session:\n",
        "      len_data = len(x_data)\n",
        "#       if tf.train.latest_checkpoint(checkpoint_path):\n",
        "#         print(\"Restore checkpoint.\")\n",
        "#         saver = tf.train.Saver()\n",
        "#         saver.restore(session, tf.train.latest_checkpoint(checkpoint_path))\n",
        "#       else:\n",
        "      session.run(tf.global_variables_initializer())\n",
        "       #print(\"No checkpoint found! start a new training...\")\n",
        "      for epoch in range(1, num_eponches + 1):\n",
        "                epoch_start_time = time.time()\n",
        "                for batch_start in range(0, len_data, batch_size):\n",
        "                    optimiser_value, loss_value = session.run([self.train_op,self.loss], feed_dict={\n",
        "                        self.x: self.next_batch(x_data, batch_start, batch_size),\n",
        "                        self.y: self.next_batch(y_data, batch_start, batch_size)})\n",
        "                    sys.stdout.write(\n",
        "                        \"\\rEpoch: {}/{}, Batch: {}/{}, Training loss: {}, Perplexity: {}\".format(\n",
        "                            epoch,\n",
        "                            num_eponches,\n",
        "                            (batch_start / batch_size) + 1,\n",
        "                            len_data / batch_size,\n",
        "                            np.mean(loss_value),\n",
        "                            tf.exp(loss_value).eval()\n",
        "                        )\n",
        "                    )\n",
        "                    sys.stdout.flush()\n",
        "                    \n",
        "                    #self.generate_text_from_trained_data(splited_data,prediction_length,session)\n",
        "                print (\" Time: {} s\".format(time.time() - epoch_start_time))\n",
        "                self.generate_text_from_trained_data(splited_data,prediction_length,session)\n",
        "      model_name = \"biLSTM_language_model.ckpt\"\n",
        "      saver.save(session, checkpoint_path + model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLJWWYGOmRO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 10\n",
        "hidden_nodes=1024\n",
        "learning_rate =0.01\n",
        "total_epoches = 500\n",
        "batch_size = 180\n",
        "\n",
        "data_split_by_word = gutenberg.words('shakespeare-macbeth.txt')\n",
        "train_x,train_y = prepare_data(data_split_by_word,10,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3NWk946S6hh",
        "colab_type": "code",
        "outputId": "8cb154db-2cc8-45e3-df14-a0d6fbc121ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_integers = [[dictionary[str(data_split_by_word[i])] for i in range(0, 10)]]\n",
        "#x_integers = np.reshape(np.array(x_integers), [-1, 10])[0]\n",
        "x_integers\n",
        "np.array(x_integers)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 574,   25, 1517,    7,   44,   83, 1518, 1519, 1520,  575]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd13-hA7FMLU",
        "colab_type": "code",
        "outputId": "e15ec736-430f-488d-f377-51676447fb6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "lstm_model = LSTM(max_len,embedding_size,hidden_nodes,learning_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-8a9e8f94e2c5>:23: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLb3IsbtWpvY",
        "colab_type": "code",
        "outputId": "dac0f9aa-9ef7-424d-ecfc-b7089865e76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5134
        }
      },
      "source": [
        "lstm_model.train(data_split_by_word,train_x,train_y,total_epoches,batch_size,10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No checkpoint found! start a new training...\n",
            "Epoch: 1/100, Batch: 129.0/128.5, Training loss: 6.878818035125732 Time: 7.447726726531982 s\n",
            " Result:\n",
            "beene , my sences would haue cool ' d To , , , , and and , , and and ,\n",
            "Epoch: 2/100, Batch: 129.0/128.5, Training loss: 6.5784173011779785 Time: 7.08791971206665 s\n",
            " Result:\n",
            "which I see before me , The Handle toward my , , and and , , and and , , ,\n",
            "Epoch: 3/100, Batch: 129.0/128.5, Training loss: 6.379707336425781 Time: 7.1255528926849365 s\n",
            " Result:\n",
            "transported me beyond This ignorant present , and I feele , , and and , , , and and , ,\n",
            "Epoch: 4/100, Batch: 129.0/128.5, Training loss: 6.214592933654785 Time: 7.133366584777832 s\n",
            " Result:\n",
            ": Lay on Macduffe , And damn ' d be , , and and , , and , and , ,\n",
            "Epoch: 5/100, Batch: 129.0/128.5, Training loss: 6.066814422607422 Time: 7.092589378356934 s\n",
            " Result:\n",
            "the loyaltie I owe , In doing it , payes , , and and , , and , , and ,\n",
            "Epoch: 6/100, Batch: 129.0/128.5, Training loss: 5.929080009460449 Time: 7.124552249908447 s\n",
            " Result:\n",
            "Macb . Well then , Now haue you consider ' d d , , , and and , , and ,\n",
            "Epoch: 7/100, Batch: 129.0/128.5, Training loss: 5.797496795654297 Time: 7.17871356010437 s\n",
            " Result:\n",
            "would ' st highly , That would ' st thou , , and , and , , and , , and\n",
            "Epoch: 8/100, Batch: 129.0/128.5, Training loss: 5.669866561889648 Time: 7.19807505607605 s\n",
            " Result:\n",
            "For sundry weightie Reasons 2 . Murth . We shall , , and and , , and , , and ,\n",
            "Epoch: 9/100, Batch: 129.0/128.5, Training loss: 5.544935703277588 Time: 7.197184324264526 s\n",
            " Result:\n",
            "become a man , Who dares do more , is , , and and , , and , , and ,\n",
            "Epoch: 10/100, Batch: 129.0/128.5, Training loss: 5.42197322845459 Time: 7.251838207244873 s\n",
            " Result:\n",
            "that doe sound so faire ? i ' th ' d d , , and and , , and , and\n",
            "Epoch: 11/100, Batch: 129.0/128.5, Training loss: 5.300514221191406 Time: 7.274482011795044 s\n",
            " Result:\n",
            "do lacke you Macb . I do forget : Do , , and and , to , and , , and\n",
            "Epoch: 12/100, Batch: 129.0/128.5, Training loss: 5.180237293243408 Time: 7.271220922470093 s\n",
            " Result:\n",
            "Hath bene but for a wayward Sonne , Spightfull , and and , , and , to , and , ,\n",
            "Epoch: 13/100, Batch: 129.0/128.5, Training loss: 5.060909271240234 Time: 7.2847912311553955 s\n",
            " Result:\n",
            "' d their Possets , That Death and Nature doe , , and , and , , and , what ,\n",
            "Epoch: 14/100, Batch: 129.0/128.5, Training loss: 4.9423909187316895 Time: 7.313117980957031 s\n",
            " Result:\n",
            "sightlesse Curriors of the Ayre , Shall blow the horrid , , and and , , and , and what ,\n",
            "Epoch: 15/100, Batch: 129.0/128.5, Training loss: 4.824618816375732 Time: 7.36532998085022 s\n",
            " Result:\n",
            "but can perceiue no truth in your report . When , , and and , , and , what , ,\n",
            "Epoch: 16/100, Batch: 129.0/128.5, Training loss: 4.707603454589844 Time: 7.342791557312012 s\n",
            " Result:\n",
            "Thanes , And mingle with the English Epicures , The and and , , and to , which , and ,\n",
            "Epoch: 17/100, Batch: 129.0/128.5, Training loss: 4.591422080993652 Time: 7.344564914703369 s\n",
            " Result:\n",
            "as well , And set me vp in hope . your your your your your your When Kings Kings shall shall\n",
            "Epoch: 18/100, Batch: 129.0/128.5, Training loss: 4.476213455200195 Time: 7.395341396331787 s\n",
            " Result:\n",
            "intombe , When liuing Light should kisse it ? Old . . What What ' s ? ? . . I\n",
            "Epoch: 19/100, Batch: 129.0/128.5, Training loss: 4.3621649742126465 Time: 7.386350393295288 s\n",
            " Result:\n",
            ": So all haile Macbeth , and Banquo 1 . to to , and , and , , and one to\n",
            "Epoch: 20/100, Batch: 129.0/128.5, Training loss: 4.249515056610107 Time: 7.364987850189209 s\n",
            " Result:\n",
            "my Lord is often thus , And hath beene from , , and and , to , which , and ,\n",
            "Epoch: 21/100, Batch: 129.0/128.5, Training loss: 4.138526916503906 Time: 7.380120515823364 s\n",
            " Result:\n",
            "full of sound and fury Signifying nothing . Enter a . . . Enter . . three . . . three\n",
            "Epoch: 22/100, Batch: 129.0/128.5, Training loss: 4.029471397399902 Time: 7.388896942138672 s\n",
            " Result:\n",
            "Enemie of Man , To make them Kings , the and and . . I Exeunt , and , and .\n",
            "Epoch: 23/100, Batch: 129.0/128.5, Training loss: 3.9226067066192627 Time: 7.4436914920806885 s\n",
            " Result:\n",
            "Make it their Walke . Enter Banquo and Fleans , . . I Enter , and , and . . I\n",
            "Epoch: 24/100, Batch: 129.0/128.5, Training loss: 3.8181512355804443 Time: 7.408724308013916 s\n",
            " Result:\n",
            "Lady . Thou ' rt mad to say it . , , and and , to , which , and ,\n",
            "Epoch: 25/100, Batch: 129.0/128.5, Training loss: 3.7162699699401855 Time: 7.396589756011963 s\n",
            " Result:\n",
            "stay vpon your leysure Macb . Giue me your fauour . . I I am not , and , and ,\n",
            "Epoch: 26/100, Batch: 129.0/128.5, Training loss: 3.617065191268921 Time: 7.430197238922119 s\n",
            " Result:\n",
            "harme ? What are these faces ? Enter Murtherers . . . Enter Enter . . three . . . How\n",
            "Epoch: 27/100, Batch: 129.0/128.5, Training loss: 3.520585060119629 Time: 7.407964706420898 s\n",
            " Result:\n",
            ", to top Macbeth Mal . I grant him Bloody , , and To , as , and , and ,\n",
            "Epoch: 28/100, Batch: 129.0/128.5, Training loss: 3.426849365234375 Time: 7.4092116355896 s\n",
            " Result:\n",
            "scorne Death , and beare His hopes ' boue Wisedome . . Exeunt . Alarums . Enter . . Enter Macbeth\n",
            "Epoch: 29/100, Batch: 129.0/128.5, Training loss: 3.3358540534973145 Time: 7.460172891616821 s\n",
            " Result:\n",
            "shall treade vpon the Tyrants head , Or weare it , , and thou ' d , , and , and\n",
            "Epoch: 30/100, Batch: 129.0/128.5, Training loss: 3.247596502304077 Time: 7.479415416717529 s\n",
            " Result:\n",
            "come , Discomfort swells : Marke King of Scotland , and and . . Enter Macbeth . . . Sey .\n",
            "Epoch: 31/100, Batch: 129.0/128.5, Training loss: 3.1620750427246094 Time: 7.433428049087524 s\n",
            " Result:\n",
            "rung Nights yawning Peale , There shall be done a , , and and what , what I else , would\n",
            "Epoch: 32/100, Batch: 129.0/128.5, Training loss: 3.0792863368988037 Time: 7.432217597961426 s\n",
            " Result:\n",
            "to you Macb . The Rest is Labor , which and and . Macb . . I Sey , and ,\n",
            "Epoch: 33/100, Batch: 129.0/128.5, Training loss: 2.999225616455078 Time: 7.441330909729004 s\n",
            " Result:\n",
            "you straight : abide within , It is concluded : ' ' d d , to the of , and ,\n",
            "Epoch: 34/100, Batch: 129.0/128.5, Training loss: 2.9218711853027344 Time: 7.447149038314819 s\n",
            " Result:\n",
            "Thou seest the Heauens , as troubled with mans Act , , and what what , what on time time time\n",
            "Epoch: 35/100, Batch: 129.0/128.5, Training loss: 2.847188949584961 Time: 7.446915626525879 s\n",
            " Result:\n",
            "Banq . Very gladly Macb . Till then enough : My My ' s l be , and , Seyward ,\n",
            "Epoch: 36/100, Batch: 129.0/128.5, Training loss: 2.7751195430755615 Time: 7.46413254737854 s\n",
            " Result:\n",
            "their Foundations : Though the treasure Of Natures Germaine , and and , And then , take to all , and\n",
            "Epoch: 37/100, Batch: 129.0/128.5, Training loss: 2.7055909633636475 Time: 7.449182748794556 s\n",
            " Result:\n",
            ", and foule is faire , Houer through the fogge of of , and , That a , which which ,\n",
            "Epoch: 38/100, Batch: 129.0/128.5, Training loss: 2.638517141342163 Time: 7.451295614242554 s\n",
            " Result:\n",
            "such a murther is La . My worthy Lord Your Your Your do do do do do do do do do\n",
            "Epoch: 39/100, Batch: 129.0/128.5, Training loss: 2.57381534576416 Time: 7.409373998641968 s\n",
            " Result:\n",
            ", Stand aye accursed in the Kalender . Come in , , and the day , the day , and ,\n",
            "Epoch: 40/100, Batch: 129.0/128.5, Training loss: 2.511408567428589 Time: 7.448862075805664 s\n",
            " Result:\n",
            "thy wholsome dayes againe ? Since that the truest Issue , , and As , come , and , and ,\n",
            "Epoch: 41/100, Batch: 129.0/128.5, Training loss: 2.4512298107147217 Time: 7.408041000366211 s\n",
            " Result:\n",
            "Why , the honest men Son . Then the Liars , , and perillous , come , and one , must\n",
            "Epoch: 42/100, Batch: 129.0/128.5, Training loss: 2.3932180404663086 Time: 7.461172103881836 s\n",
            " Result:\n",
            "That strike beside vs Sey . Enter Sir , the , , and , , and what , what one ,\n",
            "Epoch: 43/100, Batch: 129.0/128.5, Training loss: 2.337308168411255 Time: 7.445371627807617 s\n",
            " Result:\n",
            "? Ile see no more : And yet the eighth ' ' d d , the the of of , still\n",
            "Epoch: 44/100, Batch: 129.0/128.5, Training loss: 2.2834222316741943 Time: 7.5456602573394775 s\n",
            " Result:\n",
            "so depart . A shew of eight Kings , and come come , But , which , and one , what\n",
            "Epoch: 45/100, Batch: 129.0/128.5, Training loss: 2.231455087661743 Time: 7.445885896682739 s\n",
            " Result:\n",
            "That man may question ? you seeme to vnderstand me , , and By one , I on , and ,\n",
            "Epoch: 46/100, Batch: 129.0/128.5, Training loss: 2.1812775135040283 Time: 7.459981203079224 s\n",
            " Result:\n",
            "Fire burne , and Cauldron bubble 2 Fillet of a , , and , and , which , which must ,\n",
            "Epoch: 47/100, Batch: 129.0/128.5, Training loss: 2.132744073867798 Time: 7.411426067352295 s\n",
            " Result:\n",
            "' darke : Liuer of Blaspheming Iew , Gall of , , and all , and , and one to ,\n",
            "Epoch: 48/100, Batch: 129.0/128.5, Training loss: 2.0857093334198 Time: 7.461556673049927 s\n",
            " Result:\n",
            "knowledge , dearest Chuck , Till thou applaud the deed , , and , seeling seeling , , and Scotland fight\n",
            "Epoch: 49/100, Batch: 129.0/128.5, Training loss: 2.0400333404541016 Time: 7.481227874755859 s\n",
            " Result:\n",
            ", Macbeth , Macbeth : Beware Macduffe , Beware the , , and Thane , we what , to , and\n",
            "Epoch: 50/100, Batch: 129.0/128.5, Training loss: 1.9956051111221313 Time: 7.490804433822632 s\n",
            " Result:\n",
            "Banquo , Our Royall Master ' s murther ' d , , and We , are , and , must ,\n",
            "Epoch: 51/100, Batch: 129.0/128.5, Training loss: 1.9523342847824097 Time: 7.532428503036499 s\n",
            " Result:\n",
            "to my Nature . I am yet Vnknowne to Woman , , and to one one , that must Sey ,\n",
            "Epoch: 52/100, Batch: 129.0/128.5, Training loss: 1.9101474285125732 Time: 7.478157997131348 s\n",
            " Result:\n",
            "it , but still keepe My Bosome franchis ' d , , and more , I haue , would and ,\n",
            "Epoch: 53/100, Batch: 129.0/128.5, Training loss: 1.8689900636672974 Time: 7.455255031585693 s\n",
            " Result:\n",
            ". What should be spoken here , Where our Fate , , and , and what what what done done done\n",
            "Epoch: 54/100, Batch: 129.0/128.5, Training loss: 1.8288087844848633 Time: 7.425706386566162 s\n",
            " Result:\n",
            ". 1 . Why how now Hecat , you looke , , and Leuie , what what , day , do\n",
            "Epoch: 55/100, Batch: 129.0/128.5, Training loss: 1.7895472049713135 Time: 7.450755834579468 s\n",
            " Result:\n",
            "bare - fac ' d power sweepe him from my sight sight , and bid bid downfall ) . Where Here\n",
            "Epoch: 56/100, Batch: 129.0/128.5, Training loss: 1.7511427402496338 Time: 7.509634256362915 s\n",
            " Result:\n",
            ", And little is to do Malc . We haue . . beside beside ' s there , That not be\n",
            "Epoch: 57/100, Batch: 129.0/128.5, Training loss: 1.7135162353515625 Time: 7.466523885726929 s\n",
            " Result:\n",
            ", if thou haue it ; And that which rather , , and I haue a ' d , and to\n",
            "Epoch: 58/100, Batch: 129.0/128.5, Training loss: 1.676573634147644 Time: 7.444373846054077 s\n",
            " Result:\n",
            ". Were such things here , as we doe speake , , and we , and to what . Exeunt .\n",
            "Epoch: 59/100, Batch: 129.0/128.5, Training loss: 1.640223741531372 Time: 7.2351367473602295 s\n",
            " Result:\n",
            "and not the deed , Confounds vs : hearke : But But , all all be Seyward , and , And\n",
            "Epoch: 60/100, Batch: 129.0/128.5, Training loss: 1.6043626070022583 Time: 7.2362775802612305 s\n",
            " Result:\n",
            "you , to deuoure so many As will to Greatnesse , , and themselues , one , and trouble trouble ,\n",
            "Epoch: 61/100, Batch: 129.0/128.5, Training loss: 1.5688914060592651 Time: 7.211263179779053 s\n",
            " Result:\n",
            "In Thunder , Lightning , or in Raine ? 2 . . Exeunt . Secunda . and . Enter . Enter\n",
            "Epoch: 62/100, Batch: 129.0/128.5, Training loss: 1.5337154865264893 Time: 7.236318111419678 s\n",
            " Result:\n",
            "I say I saw , But know not how to doo doo , Macb . what Of , and else ,\n",
            "Epoch: 63/100, Batch: 129.0/128.5, Training loss: 1.498753547668457 Time: 7.242811679840088 s\n",
            " Result:\n",
            ", on whom I built An absolute Trust . Enter Macbeth Macbeth . Rosse . he , and dead . O\n",
            "Epoch: 64/100, Batch: 129.0/128.5, Training loss: 1.4639286994934082 Time: 7.265248775482178 s\n",
            " Result:\n",
            "shall make Honor for you Banq . So I lose , , and , ha , what , what , ,\n",
            "Epoch: 65/100, Batch: 129.0/128.5, Training loss: 1.4291778802871704 Time: 7.268093585968018 s\n",
            " Result:\n",
            "your paines are registred , Where euery day I turne , , and , When , , and , , and\n",
            "Epoch: 66/100, Batch: 129.0/128.5, Training loss: 1.394446611404419 Time: 7.219717979431152 s\n",
            " Result:\n",
            "would not betray The Deuill to his Fellow , and forsworne forsworne . Then . At win win . Dames steps\n",
            "Epoch: 67/100, Batch: 129.0/128.5, Training loss: 1.3596864938735962 Time: 7.24531888961792 s\n",
            " Result:\n",
            "o ' th ' whole Table , And to our deere deere ' t , we we againe againe , were\n",
            "Epoch: 68/100, Batch: 129.0/128.5, Training loss: 1.3248597383499146 Time: 7.254031419754028 s\n",
            " Result:\n",
            ": I had else beene perfect ; Whole as the , , and augment the men Liars , and boue call\n",
            "Epoch: 69/100, Batch: 129.0/128.5, Training loss: 1.2899305820465088 Time: 7.492334842681885 s\n",
            " Result:\n",
            "and hang vp them Wife . Now God helpe thee , , and Monkie . But on , and ! .\n",
            "Epoch: 70/100, Batch: 129.0/128.5, Training loss: 1.254870891571045 Time: 7.390685319900513 s\n",
            " Result:\n",
            "owne demerits , but for mine Fell slaughter on their , , would would , and vnprouokes a , what what\n",
            "Epoch: 71/100, Batch: 129.0/128.5, Training loss: 1.2196539640426636 Time: 7.392865180969238 s\n",
            " Result:\n",
            "d to beare my part , Or shew the glory of of our . My ioyntly , Wip , and to\n",
            "Epoch: 72/100, Batch: 129.0/128.5, Training loss: 1.1842576265335083 Time: 7.422159671783447 s\n",
            " Result:\n",
            "Offrings : and wither ' d Murther , Alarum ' d d , Treason Treason Treason Treason times times times times\n",
            "Epoch: 73/100, Batch: 129.0/128.5, Training loss: 1.1486746072769165 Time: 7.419138431549072 s\n",
            " Result:\n",
            "' th ' Sword His Wife , his Babes , and and leaue . leaue . Ile vnaccompanied , and station\n",
            "Epoch: 74/100, Batch: 129.0/128.5, Training loss: 1.112894892692566 Time: 7.399747848510742 s\n",
            " Result:\n",
            "flawes and starts ( Impostors to true feare ) would well well , womans do , will will , must must\n",
            "Epoch: 75/100, Batch: 129.0/128.5, Training loss: 1.076926589012146 Time: 7.427244186401367 s\n",
            " Result:\n",
            "else o ' re - leape , For in my , , which Heauens , all must ' d . to\n",
            "Epoch: 76/100, Batch: 129.0/128.5, Training loss: 1.0407835245132446 Time: 7.388338565826416 s\n",
            " Result:\n",
            "shut Doct . What is it she do ' s , , Looke nor , nor nor , nor nor Buffets\n",
            "Epoch: 77/100, Batch: 129.0/128.5, Training loss: 1.0045020580291748 Time: 7.390771389007568 s\n",
            " Result:\n",
            "this report Hath so exasperate their King , that hee Those Those , Then Alas ' d Disease Disease , would\n",
            "Epoch: 78/100, Batch: 129.0/128.5, Training loss: 0.968188464641571 Time: 7.424725770950317 s\n",
            " Result:\n",
            ", Thanes , And you whose places are the nearest Pious Pious , Shall , light , Thanes his Thanes againe\n",
            "Epoch: 79/100, Batch: 129.0/128.5, Training loss: 0.9320166110992432 Time: 7.4082725048065186 s\n",
            " Result:\n",
            "Banquo ' s then Macb . ' Tis better thee , , & ' thou Augures , and desires , desires\n",
            "Epoch: 80/100, Batch: 129.0/128.5, Training loss: 0.8959771990776062 Time: 7.415812969207764 s\n",
            " Result:\n",
            "young Seyward . Y . Sey . What is thy name name , Balme ' t worth worth worth . Be\n",
            "Epoch: 81/100, Batch: 129.0/128.5, Training loss: 0.8596634864807129 Time: 7.409676790237427 s\n",
            " Result:\n",
            "thy good Truth , and Honor . Diuellish Macbeth , and and one , by clipt ' t to Day ,\n",
            "Epoch: 82/100, Batch: 129.0/128.5, Training loss: 0.8228707313537598 Time: 7.412034749984741 s\n",
            " Result:\n",
            "and Slaue Mes . Let me endure your wrath , if if ' be his Will For ' lesse lesse lesse\n",
            "Epoch: 83/100, Batch: 129.0/128.5, Training loss: 0.7856967449188232 Time: 7.426023006439209 s\n",
            " Result:\n",
            "againe With twenty mortall murthers on their crownes , And vs vs , like not descended descended hearers their pillowes ,\n",
            "Epoch: 84/100, Batch: 129.0/128.5, Training loss: 0.7467734217643738 Time: 7.383526086807251 s\n",
            " Result:\n",
            "bene , That when the Braines were out , the would would we we haue there naked play Sey Sey ,\n",
            "Epoch: 85/100, Batch: 129.0/128.5, Training loss: 0.7110463380813599 Time: 7.408484220504761 s\n",
            " Result:\n",
            "not . If you can looke into the Seedes of , , And which which Arabia will Arabia not gate Line\n",
            "Epoch: 86/100, Batch: 129.0/128.5, Training loss: 0.7292865514755249 Time: 7.383010387420654 s\n",
            " Result:\n",
            "? Fleance . The Moone is downe : I haue , , and do , Seyward , to , & ,\n",
            "Epoch: 87/100, Batch: 129.0/128.5, Training loss: 0.7425765991210938 Time: 7.412640810012817 s\n",
            " Result:\n",
            "' s our chiefe Guest La . If he had , , and His ' d , Steep to Care ,\n",
            "Epoch: 88/100, Batch: 129.0/128.5, Training loss: 0.6227536201477051 Time: 7.446199655532837 s\n",
            " Result:\n",
            "- Gowne , least occasion call vs , And shew vs vs be Watchers not we poorely poorely poorely poorely lodg\n",
            "Epoch: 89/100, Batch: 129.0/128.5, Training loss: 0.6530341506004333 Time: 7.352341651916504 s\n",
            " Result:\n",
            "? Faith here ' s an Equiuocator , that could sweare sweare both the Scales eyther eyther eyther eyther Sir guesse\n",
            "Epoch: 90/100, Batch: 129.0/128.5, Training loss: 0.6729860305786133 Time: 7.385267019271851 s\n",
            " Result:\n",
            "Peale , There shall be done a deed of dreadfull . . Let Sey , Heare not , which which ,\n",
            "Epoch: 91/100, Batch: 129.0/128.5, Training loss: 0.5221027731895447 Time: 7.381977081298828 s\n",
            " Result:\n",
            "' d Lady . Woe , alas : What , any any ' s Ban , and yet fill fill fill\n",
            "Epoch: 92/100, Batch: 129.0/128.5, Training loss: 0.672477662563324 Time: 7.398753881454468 s\n",
            " Result:\n",
            "sleepe , and giuing him the Lye , leaues him Macd Macd . I Drinke Drinke Scotland thee My things such\n",
            "Epoch: 93/100, Batch: 129.0/128.5, Training loss: 0.4891677498817444 Time: 7.3653905391693115 s\n",
            " Result:\n",
            "now ? What Newes ? La . He has almost supt supt . haue not Wife She She Doome Doome eyther\n",
            "Epoch: 94/100, Batch: 129.0/128.5, Training loss: 0.4479699730873108 Time: 7.3936662673950195 s\n",
            " Result:\n",
            "Exeunt . marching . Scaena Tertia . Enter Macbeth , and and . Sey . watching watching Birnane , do do\n",
            "Epoch: 95/100, Batch: 129.0/128.5, Training loss: 0.47222036123275757 Time: 7.317404747009277 s\n",
            " Result:\n",
            ". So shall I Loue , and so I pray , , That Do Do to smell smell their Royall .\n",
            "Epoch: 96/100, Batch: 129.0/128.5, Training loss: 0.5085107684135437 Time: 7.374816179275513 s\n",
            " Result:\n",
            "the vndivulg ' d pretence , I fight Of Treasonous Mallice Mallice are owne are : if if carelesse his his\n",
            "Epoch: 97/100, Batch: 129.0/128.5, Training loss: 0.39400067925453186 Time: 7.330626964569092 s\n",
            " Result:\n",
            "Euery one Son . Who must hang them ? Wife . . Enter King , and lodg lodg Macb . Let\n",
            "Epoch: 98/100, Batch: 129.0/128.5, Training loss: 0.3732888996601105 Time: 7.387310981750488 s\n",
            " Result:\n",
            "where there is aduantage to be giuen , Both more more more warlike flye flye flye flye shewes pillowes Hence Hence\n",
            "Epoch: 99/100, Batch: 129.0/128.5, Training loss: 0.37144172191619873 Time: 7.349762439727783 s\n",
            " Result:\n",
            "Dying , or ere they sicken Macd . Oh Relation ; ; nice too and too batter blasted blasted true intermission\n",
            "Epoch: 100/100, Batch: 129.0/128.5, Training loss: 0.3884349465370178 Time: 7.406817436218262 s\n",
            " Result:\n",
            ", no more . Senit sounded . Enter Macbeth as King King , Lenox , Rosse , Lords , and Attendants\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gthajit7sHNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bi_lstm_model = biLSTM(max_len,embedding_size,hidden_nodes,learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzJTxqSsw_sQ",
        "colab_type": "code",
        "outputId": "4e1ecfe8-8f6e-4fa9-e35b-947dd9ba0877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3792
        }
      },
      "source": [
        "bi_lstm_model.train(data_split_by_word,train_x,train_y,total_epoches,batch_size,10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/500, Batch: 129.0/128.5, Training loss: 7.314228057861328, Perplexity: 1501.5123291015625 Time: 45.96108078956604 s\n",
            " Result:\n",
            "she speaks , I will set downe what comes from , , and to our , and , and to ,\n",
            "Epoch: 2/500, Batch: 129.0/128.5, Training loss: 6.37598991394043, Perplexity: 587.5667724609375 Time: 47.18421769142151 s\n",
            " Result:\n",
            "? Son . As Birds do Mother Wife . What , , and he , and , and , and ,\n",
            "Epoch: 3/500, Batch: 129.0/128.5, Training loss: 5.715014457702637, Perplexity: 303.3885803222656 Time: 50.46971917152405 s\n",
            " Result:\n",
            "Spirits , That tend on mortall thoughts , vnsex me to to . Exeunt . . Enter . . Enter .\n",
            "Epoch: 4/500, Batch: 129.0/128.5, Training loss: 5.203893184661865, Perplexity: 181.97933959960938 Time: 52.58178639411926 s\n",
            " Result:\n",
            "Drinke , Sir , is a great prouoker of three . . Exeunt . Scena . Enter . Enter . .\n",
            "Epoch: 5/500, Batch: 129.0/128.5, Training loss: 4.787283897399902, Perplexity: 119.97505950927734 Time: 55.74652028083801 s\n",
            " Result:\n",
            "that seemes to speake things strange Rosse . God saue . . Exeunt . Scena . Enter . Enter . .\n",
            "Epoch: 6/500, Batch: 129.0/128.5, Training loss: 4.437389373779297, Perplexity: 84.55391693115234 Time: 57.62531304359436 s\n",
            " Result:\n",
            "Spurre ) hath holp him To his home before vs , , and , and , and , and , and\n",
            "Epoch: 7/500, Batch: 129.0/128.5, Training loss: 4.135897159576416, Perplexity: 62.54568099975586 Time: 60.493921756744385 s\n",
            " Result:\n",
            "of goodnesse Be like our warranted Quarrell . Why are , , and , and , and what we , and\n",
            "Epoch: 8/500, Batch: 129.0/128.5, Training loss: 3.871243953704834, Perplexity: 48.00205993652344 Time: 62.60821795463562 s\n",
            " Result:\n",
            "? Let this pernitious houre , Stand aye accursed in . . Exeunt . Scena . Enter . Enter Malcolme .\n",
            "Epoch: 9/500, Batch: 129.0/128.5, Training loss: 3.636486053466797, Perplexity: 37.95821762084961 Time: 64.84860634803772 s\n",
            " Result:\n",
            ": and wither ' d Murther , Alarum ' d to to , and , and , and Seyward , and\n",
            "Epoch: 10/500, Batch: 129.0/128.5, Training loss: 3.4268317222595215, Perplexity: 30.778972625732422 Time: 67.31544947624207 s\n",
            " Result:\n",
            "Beare - like I must fight the course . What ' ' s borne . Enter Macbeth , and , and\n",
            "Epoch: 11/500, Batch: 129.0/128.5, Training loss: 3.238142251968384, Perplexity: 25.486330032348633 Time: 70.24443340301514 s\n",
            " Result:\n",
            "Who did strike out the Light ? 1 . Was , , and , and Attendants , and what we ,\n",
            "Epoch: 12/500, Batch: 129.0/128.5, Training loss: 3.0667412281036377, Perplexity: 21.471817016601562 Time: 72.19501066207886 s\n",
            " Result:\n",
            "Trifle King . There ' s no Art , To , , and the , and , and Seyward , and\n",
            "Epoch: 13/500, Batch: 129.0/128.5, Training loss: 2.909719467163086, Perplexity: 18.35165023803711 Time: 74.78335309028625 s\n",
            " Result:\n",
            "Demand 3 Wee ' l answer 1 Say , if , , and , and what wee , and , and\n",
            "Epoch: 14/500, Batch: 129.0/128.5, Training loss: 2.764958381652832, Perplexity: 15.878378868103027 Time: 77.98789882659912 s\n",
            " Result:\n",
            "th ' Sword His Wife , his Babes , and , , and Seyward , and , and Angus , and\n",
            "Epoch: 15/500, Batch: 129.0/128.5, Training loss: 2.631002426147461, Perplexity: 13.887683868408203 Time: 80.43999934196472 s\n",
            " Result:\n",
            "your Noble strength , to thinke So braine - sickly , , As ' d me to . Enter . Enter\n",
            "Epoch: 16/500, Batch: 129.0/128.5, Training loss: 2.506951332092285, Perplexity: 12.267473220825195 Time: 83.2457959651947 s\n",
            " Result:\n",
            ": While then , God be with you . Exeunt . . Scena Quarta . Enter . Enter Malcolme and ,\n",
            "Epoch: 17/500, Batch: 129.0/128.5, Training loss: 2.3923275470733643, Perplexity: 10.938924789428711 Time: 86.85583114624023 s\n",
            " Result:\n",
            ", and stayes for me . Sing within . Come , , and , and Seyward , and , Seyward ,\n",
            "Epoch: 18/500, Batch: 129.0/128.5, Training loss: 2.2868988513946533, Perplexity: 9.844361305236816 Time: 89.06749224662781 s\n",
            " Result:\n",
            "shadowes , so depart . A shew of eight Kings . . What is he is dead , and to .\n",
            "Epoch: 19/500, Batch: 129.0/128.5, Training loss: 2.190437078475952, Perplexity: 8.939119338989258 Time: 91.79647088050842 s\n",
            " Result:\n",
            "Our free Hearts each to other Banq . Very gladly be be done , and then more then more then heere\n",
            "Epoch: 20/500, Batch: 129.0/128.5, Training loss: 2.10253643989563, Perplexity: 8.186909675598145 Time: 95.96338176727295 s\n",
            " Result:\n",
            "hauing no witnesse to confirme my speech . Enter Lady . . My , and , and Seyward , and ,\n",
            "Epoch: 21/500, Batch: 129.0/128.5, Training loss: 2.022542953491211, Perplexity: 7.55751895904541 Time: 99.2429940700531 s\n",
            " Result:\n",
            "troubled with thicke - comming Fancies That keepe her from her her hands , and , and , Seyward , and\n",
            "Epoch: 22/500, Batch: 129.0/128.5, Training loss: 1.9495930671691895, Perplexity: 7.025827884674072 Time: 101.0814208984375 s\n",
            " Result:\n",
            "setting downe befor ' t Malc . ' Tis his Chamber Chamber , and with , and , That , and\n",
            "Epoch: 23/500, Batch: 129.0/128.5, Training loss: 1.882742166519165, Perplexity: 6.571500301361084 Time: 105.75917339324951 s\n",
            " Result:\n",
            "needs she the Diuine , then the Physitian : God God God blesse vs , and , and Seyward , and\n",
            "Epoch: 24/500, Batch: 129.0/128.5, Training loss: 1.8211042881011963, Perplexity: 6.178677558898926 Time: 107.92401719093323 s\n",
            " Result:\n",
            "? you seeme to vnderstand me , By each at once once to each one , and each one Am ,\n",
            "Epoch: 25/500, Batch: 129.0/128.5, Training loss: 1.7639261484146118, Perplexity: 5.835302829742432 Time: 108.97859334945679 s\n",
            " Result:\n",
            "d him Father to a Line of Kings . Vpon , , and a - , and , and Seyward slaine\n",
            "Epoch: 26/500, Batch: 129.0/128.5, Training loss: 1.7105787992477417, Perplexity: 5.532162666320801 Time: 114.59887790679932 s\n",
            " Result:\n",
            "with feare . Enter Seruant . The diuell damne thee . . Thou . Enter Macduffe , and with . Enter\n",
            "Epoch: 27/500, Batch: 129.0/128.5, Training loss: 1.6605299711227417, Perplexity: 5.262098789215088 Time: 117.95517468452454 s\n",
            " Result:\n",
            "cleane ? No more o ' that my Lord , And And with with him . Ile . comes , and\n",
            "Epoch: 28/500, Batch: 129.0/128.5, Training loss: 1.613321304321289, Perplexity: 5.019454479217529 Time: 123.05481696128845 s\n",
            " Result:\n",
            "don ' t ? Lady . Who dares receiue it , , As she two spent . Some . I pray\n",
            "Epoch: 29/500, Batch: 129.0/128.5, Training loss: 1.5685580968856812, Perplexity: 4.799722671508789 Time: 126.43037724494934 s\n",
            " Result:\n",
            "the wonder of it , came Missiues from the King , , and to see , and Attendants . Alarum .\n",
            "Epoch: 30/500, Batch: 129.0/128.5, Training loss: 1.5259063243865967, Perplexity: 4.599309921264648 Time: 130.74029445648193 s\n",
            " Result:\n",
            "rises like the issue of a King , And weares , , and to a word . . Exeunt . Scena\n",
            "Epoch: 31/500, Batch: 129.0/128.5, Training loss: 1.4850857257843018, Perplexity: 4.415343761444092 Time: 135.32427644729614 s\n",
            " Result:\n",
            "else worth all the rest : I see thee still , , and the Alarum , and , and Affaires ,\n",
            "Epoch: 32/500, Batch: 129.0/128.5, Training loss: 1.445864200592041, Perplexity: 4.245519638061523 Time: 141.49147367477417 s\n",
            " Result:\n",
            ", That palter with vs in a double sence , and and . Haile . Heere . I see , and\n",
            "Epoch: 33/500, Batch: 129.0/128.5, Training loss: 1.4080544710159302, Perplexity: 4.087994575500488 Time: 147.4239251613617 s\n",
            " Result:\n",
            "Aleppo gone , Master o ' th ' Tiger : What What ' s done , and he is dead ,\n",
            "Epoch: 34/500, Batch: 129.0/128.5, Training loss: 1.3714996576309204, Perplexity: 3.9412567615509033 Time: 151.88463759422302 s\n",
            " Result:\n",
            "to Colmekill , The Sacred Store - house of his promise promise , And wash this Fiend Hath , but for\n",
            "Epoch: 35/500, Batch: 129.0/128.5, Training loss: 1.3360743522644043, Perplexity: 3.8040807247161865 Time: 158.86040329933167 s\n",
            " Result:\n",
            "Stabs , look ' d like a Breach in Nature , , And must be that ' d with . Enter\n",
            "Epoch: 36/500, Batch: 129.0/128.5, Training loss: 1.3016763925552368, Perplexity: 3.675452947616577 Time: 165.6623182296753 s\n",
            " Result:\n",
            "visited people All swolne and Vlcerous , pittifull to the eye eye , and we haue , and to all ,\n",
            "Epoch: 37/500, Batch: 129.0/128.5, Training loss: 1.2682218551635742, Perplexity: 3.5545265674591064 Time: 172.17372226715088 s\n",
            " Result:\n",
            "and I performe vpon Th ' vnguarded Duncan ? What is is done , and Fairies how . Enter Macbeth ,\n",
            "Epoch: 38/500, Batch: 129.0/128.5, Training loss: 1.235647201538086, Perplexity: 3.4406044483184814 Time: 178.66891503334045 s\n",
            " Result:\n",
            "I heare it by the way : But I will not not found pious : But heere ' s knocking .\n",
            "Epoch: 39/500, Batch: 129.0/128.5, Training loss: 1.2038943767547607, Perplexity: 3.3330719470977783 Time: 184.09326481819153 s\n",
            " Result:\n",
            ". Well too Macd . The Tyrant ha ' s a a name Macb . Thou , none ' s no\n",
            "Epoch: 40/500, Batch: 129.0/128.5, Training loss: 1.172922968864441, Perplexity: 3.23142409324646 Time: 190.5029366016388 s\n",
            " Result:\n",
            "Horse the better , I must become a borrower of God God sweet his drop ; And Pitty be , and\n",
            "Epoch: 41/500, Batch: 129.0/128.5, Training loss: 1.142698884010315, Perplexity: 3.135218620300293 Time: 197.41328263282776 s\n",
            " Result:\n",
            "Armour on : giue me my Staffe : Seyton , come come , giue , and , Seyward , and ,\n",
            "Epoch: 42/500, Batch: 129.0/128.5, Training loss: 1.1131960153579712, Perplexity: 3.044071674346924 Time: 200.46416544914246 s\n",
            " Result:\n",
            "Sir , can you tell Where he bestowes himselfe ? . . I am not so . But . We haue\n",
            "Epoch: 43/500, Batch: 129.0/128.5, Training loss: 1.0843932628631592, Perplexity: 2.9576447010040283 Time: 207.7935631275177 s\n",
            " Result:\n",
            "badg ' d with blood , So were their Daggers , , and to each one , to scorne , and\n",
            "Epoch: 44/500, Batch: 129.0/128.5, Training loss: 1.0562747716903687, Perplexity: 2.875638484954834 Time: 216.4992561340332 s\n",
            " Result:\n",
            "Dagger of the Minde , a false Creation , Proceeding from from whence thought to thought , and to pieces ,\n",
            "Epoch: 45/500, Batch: 129.0/128.5, Training loss: 1.0288230180740356, Perplexity: 2.7977709770202637 Time: 223.19848489761353 s\n",
            " Result:\n",
            "The seruice , and the loyaltie I owe , In the the Flame where - hail . But go , And\n",
            "Epoch: 46/500, Batch: 129.0/128.5, Training loss: 1.0020207166671753, Perplexity: 2.7237801551818848 Time: 230.51917147636414 s\n",
            " Result:\n",
            "Goe get some Water , And wash this filthie Witnesse from from our Gallowgrosses ' d Vpon goes hence would Make\n",
            "Epoch: 47/500, Batch: 129.0/128.5, Training loss: 0.9758473038673401, Perplexity: 2.653414487838745 Time: 237.25219774246216 s\n",
            " Result:\n",
            "of powrefull trouble , Like a Hell - broth , whose whose Was Was he fought , to a one ,\n",
            "Epoch: 48/500, Batch: 129.0/128.5, Training loss: 0.9502798914909363, Perplexity: 2.5864334106445312 Time: 246.6030502319336 s\n",
            " Result:\n",
            "vpbraid his Faith - breach : Those he commands , and and Colours him as he burne , and him ,\n",
            "Epoch: 49/500, Batch: 129.0/128.5, Training loss: 0.9252936244010925, Perplexity: 2.522608757019043 Time: 254.8737006187439 s\n",
            " Result:\n",
            "Exeunt . fighting . Alarums . Enter Fighting , and with with . Old . Haile . Enter . Enter Seruant\n",
            "Epoch: 50/500, Batch: 129.0/128.5, Training loss: 0.9008595943450928, Perplexity: 2.4617183208465576 Time: 264.4602572917938 s\n",
            " Result:\n",
            "Constancie Hath left you vnattended . Knocke . Hearke , if if with with hidden . Macd . My Royall Father\n",
            "Epoch: 51/500, Batch: 129.0/128.5, Training loss: 0.8769499659538269, Perplexity: 2.403557538986206 Time: 271.50187587738037 s\n",
            " Result:\n",
            "haue thee , as our rarer Monsters are Painted vpon , , and what I , and to feare , and\n",
            "Epoch: 52/500, Batch: 129.0/128.5, Training loss: 0.8535395860671997, Perplexity: 2.34794282913208 Time: 281.88759541511536 s\n",
            " Result:\n",
            "wishest should be vndone . High thee hither , That I I powre Aroynt to Dunsinane , and Shall to him\n",
            "Epoch: 53/500, Batch: 129.0/128.5, Training loss: 0.8305999040603638, Perplexity: 2.2946949005126953 Time: 291.0620939731598 s\n",
            " Result:\n",
            "' em Macd . Be not a niggard of your , , and Vprore his owne , and their feare ,\n",
            "Epoch: 54/500, Batch: 129.0/128.5, Training loss: 0.8081072568893433, Perplexity: 2.243657350540161 Time: 300.61841320991516 s\n",
            " Result:\n",
            "cine of the sickly Weale , And with him poure . . Exeunt . Enter . Enter Malcolme and . Mal\n",
            "Epoch: 55/500, Batch: 129.0/128.5, Training loss: 0.7860352993011475, Perplexity: 2.1946778297424316 Time: 308.9808487892151 s\n",
            " Result:\n",
            "my dread exploits : The flighty purpose neuer is o ' ' d , And for thee Coward , But no\n",
            "Epoch: 56/500, Batch: 129.0/128.5, Training loss: 0.7643610835075378, Perplexity: 2.1476218700408936 Time: 318.6474175453186 s\n",
            " Result:\n",
            "bin strangely borne . The gracious Duncan Was pittied of that that Which was had hath beene , and gaze ,\n",
            "Epoch: 57/500, Batch: 129.0/128.5, Training loss: 0.743060827255249, Perplexity: 2.102360725402832 Time: 328.50269532203674 s\n",
            " Result:\n",
            "By this great clatter , one of greatest note Seemes bruited bruited . Double . Exeunt . Scaena Tertia . Enter\n",
            "Epoch: 58/500, Batch: 129.0/128.5, Training loss: 0.7221105098724365, Perplexity: 2.0587737560272217 Time: 335.5723509788513 s\n",
            " Result:\n",
            "shall giue way . I am in blood Stept in his his Whence , and like enrages that dauntlesse Feare ,\n",
            "Epoch: 59/500, Batch: 129.0/128.5, Training loss: 0.7014890909194946, Perplexity: 2.016753673553467 Time: 346.1942346096039 s\n",
            " Result:\n",
            "in the Vessell of my Peace Onely for them , and and paid to feare , and Seyward more , and\n",
            "Epoch: 60/500, Batch: 129.0/128.5, Training loss: 0.6811758279800415, Perplexity: 1.976199984550476 Time: 353.1419999599457 s\n",
            " Result:\n",
            "st not play false , And yet would ' st wrongly wrongly winne ' d : pronounc , here Night downe\n",
            "Epoch: 61/500, Batch: 129.0/128.5, Training loss: 0.6611533164978027, Perplexity: 1.9370250701904297 Time: 362.606014251709 s\n",
            " Result:\n",
            "Ports they blow , All the Quarters that they know . . Exeunt . Actus . Enter . Seyton and .\n",
            "Epoch: 62/500, Batch: 129.0/128.5, Training loss: 0.6414041519165039, Perplexity: 1.8991457223892212 Time: 372.6885097026825 s\n",
            " Result:\n",
            "Leafe , To reade them . Let vs toward the deed deed he be come to come , and dare .\n",
            "Epoch: 63/500, Batch: 129.0/128.5, Training loss: 0.6219167709350586, Perplexity: 1.8624945878982544 Time: 381.5869109630585 s\n",
            " Result:\n",
            ", haue Napkins enow about you , here you ' le le sweat . Exit Messenger . Hayle . They haue\n",
            "Epoch: 64/500, Batch: 129.0/128.5, Training loss: 0.6026787757873535, Perplexity: 1.8270063400268555 Time: 392.01313042640686 s\n",
            " Result:\n",
            "the Lady Mal . Why doe we hold our tongues , , Ile make our Art , and to feare ,\n",
            "Epoch: 65/500, Batch: 129.0/128.5, Training loss: 0.583681046962738, Perplexity: 1.792625069618225 Time: 402.99058175086975 s\n",
            " Result:\n",
            "Breach in Nature , For Ruines wastfull entrance : there ' ' d , and Tartars lips , Brest was '\n",
            "Epoch: 66/500, Batch: 129.0/128.5, Training loss: 0.5649179220199585, Perplexity: 1.759303331375122 Time: 414.2535650730133 s\n",
            " Result:\n",
            "' st thou that Macduff denies his person At our great great bidding Did Did on second on second wake folly\n",
            "Epoch: 67/500, Batch: 129.0/128.5, Training loss: 0.5463841557502747, Perplexity: 1.7269971370697021 Time: 426.2904508113861 s\n",
            " Result:\n",
            "and Lightning . Enter three Witches . 1 . When we we thirst , And all our Our do ' s\n",
            "Epoch: 68/500, Batch: 129.0/128.5, Training loss: 0.5280764698982239, Perplexity: 1.6956675052642822 Time: 437.39336609840393 s\n",
            " Result:\n",
            "souldiers debt , He onely liu ' d but till he he was So great . My Noble as Noble Macb\n",
            "Epoch: 69/500, Batch: 2.0/128.5, Training loss: 1.3671250343322754, Perplexity: 3.9240529537200928"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-61d2882d058b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbi_lstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_split_by_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_epoches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-6d9809cb3516>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, splited_data, x_data, y_data, num_eponches, batch_size, prediction_length, checkpoint_path)\u001b[0m\n\u001b[1;32m     92\u001b[0m                             \u001b[0mlen_data\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                         )\n\u001b[1;32m     96\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \"\"\"\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5179\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5180\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5181\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jrn5Bvf062K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}